{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f005cd2",
   "metadata": {
    "papermill": {
     "duration": 0.008019,
     "end_time": "2024-04-23T16:17:07.015723",
     "exception": false,
     "start_time": "2024-04-23T16:17:07.007704",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Customizable pytorch LLM training pipeline\n",
    "\n",
    "## 왜 이 노트북을 사용하나요?\n",
    "\n",
    "많은 사람들처럼, 저는 LLMs에 대한 기술과 이해를 향상시키려고 노력하고 있습니다.\n",
    "매일 새로운 것들이 등장하고 있으며, 따라가기가 어렵습니다.\n",
    "몇 줄의 코드로 LLM을 미세 조정할 수 있는 훌륭한 리소스들이 많이 있습니다 (transformers, trl, auto-train, ...).\n",
    "하지만 이러한 준비된 라이브러리를 사용할 때는 많이 배우는 느낌이 들지 않습니다.\n",
    "그래서 저는 최종 사용자에게 모든 세부 사항을 가능한 한 많이 보여주고 사용자 정의할 수 있는 간단한 독립형 노트북을 제안합니다. 물론 바퀴를 다시 발명하지 않고요.\n",
    "\n",
    "제가 말했듯이, 저는 현재 이 모든 것을 배우고 있으며, 제 파이프라인에 오류나 쉬운 개선점이 있을 수 있습니다. 발견하시면 주저하지 말고 알려주셔서 모두가 개선할 수 있도록 도와주세요!\n",
    "\n",
    "## 사용자 정의를 위한 아이디어\n",
    "\n",
    "이 노트북을 넘어서는 몇 가지 아이디어:\n",
    "- 다른 LLM backbones 시도하기\n",
    "- 더 복잡한 prediction head 추가하기\n",
    "- 다양한 loss functions 탐색하기\n",
    "- 다른 pooling method 시도하기\n",
    "\n",
    "## 아직 작동하지 않는 것들\n",
    "이것이 어떻게 작동할 수 있는지 알려주세요!\n",
    "\n",
    "- float16, bfloat16, 또는 int8, int4로 LLM을 로드하고 훈련하기: 이 작업을 수행하기 위해 파이프라인 내부에서 무엇을 해야 하는지 아직 모르겠습니다 (메모리 필요량 측면에서 모든 것이 바뀔 것입니다).\n",
    "- 모델을 저장할 때 메모리를 절약하기 위해 peft weights + custom layers만 저장하기\n",
    "\n",
    "## 탐색할 다음 단계\n",
    "비슷한 노트북을 탐색하고 공유할 시간을 찾으려고 합니다:\n",
    "- qlora\n",
    "- torchtune\n",
    "- 그 외에 무엇이 있을까요? 댓글로 아이디어를 공유해주세요!\n",
    "\n",
    "**Happy Kaggling!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd06c1d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-23T16:17:07.032419Z",
     "iopub.status.busy": "2024-04-23T16:17:07.032013Z",
     "iopub.status.idle": "2024-04-23T16:17:27.067876Z",
     "shell.execute_reply": "2024-04-23T16:17:27.066751Z"
    },
    "papermill": {
     "duration": 20.047131,
     "end_time": "2024-04-23T16:17:27.070376",
     "exception": false,
     "start_time": "2024-04-23T16:17:07.023245",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # install latest libraries\n",
    "# ! pip install -q /kaggle/input/lal-scoring-wheels/tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl --no-deps\n",
    "# ! pip install -q /kaggle/input/lal-scoring-wheels/transformers-4.40.0-py3-none-any.whl --no-deps\n",
    "# ! pip install -q /kaggle/input/lal-scoring-wheels/peft-0.10.0-py3-none-any.whl --no-deps\n",
    "# ! pip install -q /kaggle/input/lal-scoring-wheels/accelerate-0.29.3-py3-none-any.whl --no-deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6ea505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tokenizers==0.19.1\n",
    "# !pip install transformers==4.40.0\n",
    "# !pip install peft==0.10.0\n",
    "# !pip install accelerate==0.29.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91b111b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-23T16:17:27.086747Z",
     "iopub.status.busy": "2024-04-23T16:17:27.086419Z",
     "iopub.status.idle": "2024-04-23T16:17:31.973938Z",
     "shell.execute_reply": "2024-04-23T16:17:31.973005Z"
    },
    "papermill": {
     "duration": 4.898297,
     "end_time": "2024-04-23T16:17:31.976375",
     "exception": false,
     "start_time": "2024-04-23T16:17:27.078078",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1922ea71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed_tokens.weight\n",
      "layers.0.self_attn.q_proj.weight\n",
      "layers.0.self_attn.k_proj.weight\n",
      "layers.0.self_attn.v_proj.weight\n",
      "layers.0.self_attn.o_proj.weight\n",
      "layers.0.mlp.gate_proj.weight\n",
      "layers.0.mlp.up_proj.weight\n",
      "layers.0.mlp.down_proj.weight\n",
      "layers.0.input_layernorm.weight\n",
      "layers.0.post_attention_layernorm.weight\n",
      "layers.1.self_attn.q_proj.weight\n",
      "layers.1.self_attn.k_proj.weight\n",
      "layers.1.self_attn.v_proj.weight\n",
      "layers.1.self_attn.o_proj.weight\n",
      "layers.1.mlp.gate_proj.weight\n",
      "layers.1.mlp.up_proj.weight\n",
      "layers.1.mlp.down_proj.weight\n",
      "layers.1.input_layernorm.weight\n",
      "layers.1.post_attention_layernorm.weight\n",
      "layers.2.self_attn.q_proj.weight\n",
      "layers.2.self_attn.k_proj.weight\n",
      "layers.2.self_attn.v_proj.weight\n",
      "layers.2.self_attn.o_proj.weight\n",
      "layers.2.mlp.gate_proj.weight\n",
      "layers.2.mlp.up_proj.weight\n",
      "layers.2.mlp.down_proj.weight\n",
      "layers.2.input_layernorm.weight\n",
      "layers.2.post_attention_layernorm.weight\n",
      "layers.3.self_attn.q_proj.weight\n",
      "layers.3.self_attn.k_proj.weight\n",
      "layers.3.self_attn.v_proj.weight\n",
      "layers.3.self_attn.o_proj.weight\n",
      "layers.3.mlp.gate_proj.weight\n",
      "layers.3.mlp.up_proj.weight\n",
      "layers.3.mlp.down_proj.weight\n",
      "layers.3.input_layernorm.weight\n",
      "layers.3.post_attention_layernorm.weight\n",
      "layers.4.self_attn.q_proj.weight\n",
      "layers.4.self_attn.k_proj.weight\n",
      "layers.4.self_attn.v_proj.weight\n",
      "layers.4.self_attn.o_proj.weight\n",
      "layers.4.mlp.gate_proj.weight\n",
      "layers.4.mlp.up_proj.weight\n",
      "layers.4.mlp.down_proj.weight\n",
      "layers.4.input_layernorm.weight\n",
      "layers.4.post_attention_layernorm.weight\n",
      "layers.5.self_attn.q_proj.weight\n",
      "layers.5.self_attn.k_proj.weight\n",
      "layers.5.self_attn.v_proj.weight\n",
      "layers.5.self_attn.o_proj.weight\n",
      "layers.5.mlp.gate_proj.weight\n",
      "layers.5.mlp.up_proj.weight\n",
      "layers.5.mlp.down_proj.weight\n",
      "layers.5.input_layernorm.weight\n",
      "layers.5.post_attention_layernorm.weight\n",
      "layers.6.self_attn.q_proj.weight\n",
      "layers.6.self_attn.k_proj.weight\n",
      "layers.6.self_attn.v_proj.weight\n",
      "layers.6.self_attn.o_proj.weight\n",
      "layers.6.mlp.gate_proj.weight\n",
      "layers.6.mlp.up_proj.weight\n",
      "layers.6.mlp.down_proj.weight\n",
      "layers.6.input_layernorm.weight\n",
      "layers.6.post_attention_layernorm.weight\n",
      "layers.7.self_attn.q_proj.weight\n",
      "layers.7.self_attn.k_proj.weight\n",
      "layers.7.self_attn.v_proj.weight\n",
      "layers.7.self_attn.o_proj.weight\n",
      "layers.7.mlp.gate_proj.weight\n",
      "layers.7.mlp.up_proj.weight\n",
      "layers.7.mlp.down_proj.weight\n",
      "layers.7.input_layernorm.weight\n",
      "layers.7.post_attention_layernorm.weight\n",
      "layers.8.self_attn.q_proj.weight\n",
      "layers.8.self_attn.k_proj.weight\n",
      "layers.8.self_attn.v_proj.weight\n",
      "layers.8.self_attn.o_proj.weight\n",
      "layers.8.mlp.gate_proj.weight\n",
      "layers.8.mlp.up_proj.weight\n",
      "layers.8.mlp.down_proj.weight\n",
      "layers.8.input_layernorm.weight\n",
      "layers.8.post_attention_layernorm.weight\n",
      "layers.9.self_attn.q_proj.weight\n",
      "layers.9.self_attn.k_proj.weight\n",
      "layers.9.self_attn.v_proj.weight\n",
      "layers.9.self_attn.o_proj.weight\n",
      "layers.9.mlp.gate_proj.weight\n",
      "layers.9.mlp.up_proj.weight\n",
      "layers.9.mlp.down_proj.weight\n",
      "layers.9.input_layernorm.weight\n",
      "layers.9.post_attention_layernorm.weight\n",
      "layers.10.self_attn.q_proj.weight\n",
      "layers.10.self_attn.k_proj.weight\n",
      "layers.10.self_attn.v_proj.weight\n",
      "layers.10.self_attn.o_proj.weight\n",
      "layers.10.mlp.gate_proj.weight\n",
      "layers.10.mlp.up_proj.weight\n",
      "layers.10.mlp.down_proj.weight\n",
      "layers.10.input_layernorm.weight\n",
      "layers.10.post_attention_layernorm.weight\n",
      "layers.11.self_attn.q_proj.weight\n",
      "layers.11.self_attn.k_proj.weight\n",
      "layers.11.self_attn.v_proj.weight\n",
      "layers.11.self_attn.o_proj.weight\n",
      "layers.11.mlp.gate_proj.weight\n",
      "layers.11.mlp.up_proj.weight\n",
      "layers.11.mlp.down_proj.weight\n",
      "layers.11.input_layernorm.weight\n",
      "layers.11.post_attention_layernorm.weight\n",
      "layers.12.self_attn.q_proj.weight\n",
      "layers.12.self_attn.k_proj.weight\n",
      "layers.12.self_attn.v_proj.weight\n",
      "layers.12.self_attn.o_proj.weight\n",
      "layers.12.mlp.gate_proj.weight\n",
      "layers.12.mlp.up_proj.weight\n",
      "layers.12.mlp.down_proj.weight\n",
      "layers.12.input_layernorm.weight\n",
      "layers.12.post_attention_layernorm.weight\n",
      "layers.13.self_attn.q_proj.weight\n",
      "layers.13.self_attn.k_proj.weight\n",
      "layers.13.self_attn.v_proj.weight\n",
      "layers.13.self_attn.o_proj.weight\n",
      "layers.13.mlp.gate_proj.weight\n",
      "layers.13.mlp.up_proj.weight\n",
      "layers.13.mlp.down_proj.weight\n",
      "layers.13.input_layernorm.weight\n",
      "layers.13.post_attention_layernorm.weight\n",
      "layers.14.self_attn.q_proj.weight\n",
      "layers.14.self_attn.k_proj.weight\n",
      "layers.14.self_attn.v_proj.weight\n",
      "layers.14.self_attn.o_proj.weight\n",
      "layers.14.mlp.gate_proj.weight\n",
      "layers.14.mlp.up_proj.weight\n",
      "layers.14.mlp.down_proj.weight\n",
      "layers.14.input_layernorm.weight\n",
      "layers.14.post_attention_layernorm.weight\n",
      "layers.15.self_attn.q_proj.weight\n",
      "layers.15.self_attn.k_proj.weight\n",
      "layers.15.self_attn.v_proj.weight\n",
      "layers.15.self_attn.o_proj.weight\n",
      "layers.15.mlp.gate_proj.weight\n",
      "layers.15.mlp.up_proj.weight\n",
      "layers.15.mlp.down_proj.weight\n",
      "layers.15.input_layernorm.weight\n",
      "layers.15.post_attention_layernorm.weight\n",
      "layers.16.self_attn.q_proj.weight\n",
      "layers.16.self_attn.k_proj.weight\n",
      "layers.16.self_attn.v_proj.weight\n",
      "layers.16.self_attn.o_proj.weight\n",
      "layers.16.mlp.gate_proj.weight\n",
      "layers.16.mlp.up_proj.weight\n",
      "layers.16.mlp.down_proj.weight\n",
      "layers.16.input_layernorm.weight\n",
      "layers.16.post_attention_layernorm.weight\n",
      "layers.17.self_attn.q_proj.weight\n",
      "layers.17.self_attn.k_proj.weight\n",
      "layers.17.self_attn.v_proj.weight\n",
      "layers.17.self_attn.o_proj.weight\n",
      "layers.17.mlp.gate_proj.weight\n",
      "layers.17.mlp.up_proj.weight\n",
      "layers.17.mlp.down_proj.weight\n",
      "layers.17.input_layernorm.weight\n",
      "layers.17.post_attention_layernorm.weight\n",
      "norm.weight\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModel\n",
    "\n",
    "# 모델 경로 설정\n",
    "model_path = \"/Users/petersong/code/f-lab/LoRA/models/gemma-transformers-1.1-2b-it-v1\"\n",
    "\n",
    "# 모델 로드\n",
    "model = AutoModel.from_pretrained(model_path)\n",
    "\n",
    "# state_dict의 키 출력\n",
    "for name, param in model.state_dict().items():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00caf54",
   "metadata": {
    "papermill": {
     "duration": 0.006679,
     "end_time": "2024-04-23T16:17:31.990308",
     "exception": false,
     "start_time": "2024-04-23T16:17:31.983629",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 실험 구성\n",
    "설정이 완료되면 하이퍼파라미터만 조정하여 최적의 모델을 찾으면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ccafffa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-23T16:17:32.006354Z",
     "iopub.status.busy": "2024-04-23T16:17:32.005426Z",
     "iopub.status.idle": "2024-04-23T16:17:32.019072Z",
     "shell.execute_reply": "2024-04-23T16:17:32.018342Z"
    },
    "papermill": {
     "duration": 0.023921,
     "end_time": "2024-04-23T16:17:32.021098",
     "exception": false,
     "start_time": "2024-04-23T16:17:31.997177",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Config():\n",
    "    def __init__(self):\n",
    "        # 문제와 관련된 매개변수\n",
    "        self.num_classes = 1 # 회귀를 위한 1개의 클래스\n",
    "        \n",
    "        # 네트워크와 관련된 매개변수\n",
    "        self.architecture = {\"backbone\": \"models/gemma-transforemrs-1.1-2b-it-v1\",\n",
    "                             \"params\": {}}\n",
    "        self.remove_layers = 8 # 모델을 작게 만들기 위해 제거할 레이어 수\n",
    "        self.freeze_layers = None # 훈련 매개변수 수를 줄이기 위해 고정할 레이어 수\n",
    "        self.use_lora = True\n",
    "        self.lora_config = {\"r\": 16, # 분해된 행렬의 랭크 (높을수록 메모리 절약이 적음)\n",
    "                            \"lora_alpha\": 32, # 스케일링 팩터, https://www.entrypointai.com/blog/lora-fine-tuning/에 따르면 2xr이어야 함\n",
    "                            \"lora_dropout\": 0.1, # 일반적인 dropout\n",
    "                            # 백본에 따라 모듈을 올바르게 이름 지정해야 함\n",
    "                            # attention blocks의 linear layers를 찾아야 함\n",
    "                            \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                                               \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "                           }\n",
    "        \n",
    "        self.token_info = {\"padding\" :\"longest\", # 배치는 가장 긴 시퀀스의 길이가 될 것임\n",
    "                           \"max_length\" : 256, # 로컬에서 1024로 훈련, kaggle 노트북 내에서 훈련하기 위해 낮춤\n",
    "                           \"truncation\": True,\n",
    "                           \"pad_to_multiple_of\" : 512 \n",
    "                           # 패딩을 특정 값의 배수로 맞추는 옵션. \n",
    "                           # VIDIA의 Tensor Cores와 같은 하드웨어 가속기에서는, 연산 속도를 최적화하기 위해 배치 크기나 입력 길이가 특정 배수로 정렬될 때 더 나은 성능을 발휘하는 경우가 많습니다. \n",
    "                           # 이때 pad_to_multiple_of를 사용하여 입력을 정렬할 수 있습니다\n",
    "                          }\n",
    "\n",
    "        # 훈련과 관련된 매개변수\n",
    "        self.max_epochs = 1 # 에포크 수\n",
    "\n",
    "        self.initial_lr =1e-4\n",
    "        self.optimizer_name = \"AdamW\" #\"AdamW\" # 8 bit adam AdamW8bit 시도     \n",
    "        self.optimizer_params = {\"lr\": self.initial_lr, \n",
    "                                 \"weight_decay\":1e-2\n",
    "                                }\n",
    "        self.loss_config = {\"loss_name\" : \"MSELoss\",\n",
    "                            \"reduction\":\"mean\",\n",
    "                           }\n",
    "        # OneCycleLR는 학습률을 한 사이클 동안 증가시켰다가 감소시키는 방식으로 조정하는 스케줄러\n",
    "        # 학습률을 효과적으로 조정하여 모델의 수렴 속도를 높이고, 과적합을 방지하는 데 도움을 줌\n",
    "        # OneCycleLR 스케줄러는 특히 빠른 수렴과 일반화 성능을 개선하는 데 유용한 것으로 알려져 있음\n",
    "        self.scheduler_name = \"OneCycleLR\"\n",
    "        self.steps_per_epochs = -1 # 자동으로 덮어씌워짐\n",
    "        self.scheduler_params={\n",
    "                              #학습률의 최대값을 설정합니다. 이는 optimizer_params에서 정의된 학습률을 사용\n",
    "                              \"max_lr\":self.optimizer_params[\"lr\"] if type(self.optimizer_params)==dict else self.optimizer_params[-1][\"lr\"],\n",
    "                               #초기 학습률을 max_lr / div_factor로 나눕니다. 초기에는 작은 학습률로 시작하여 안정적으로 모델을 학습시키고, 이후 학습률을 증가시키는 전략\n",
    "                               \"div_factor\":10,\n",
    "                              \"steps_per_epoch\": self.steps_per_epochs,\n",
    "                              #학습이 완료될 때 최종 학습률을 max_lr / final_div_factor로 나눕니다. 이는 학습률을 최종적으로 조정하는 데 사용됩니다.\n",
    "                              \"final_div_factor\":1e2, #1e2\n",
    "                              #학습률 조정 전략을 설정합니다. \"cos\"는 코사인 곡선을 사용하여 학습률을 조정합니다.\n",
    "                              \"anneal_strategy\":\"cos\", #\"cos\"\n",
    "                              #세 단계 학습률 조정을 사용할지 여부를 설정합니다. 세 단계 학습률 조정은 학습률을 세 단계로 나누어 조정합니다.\n",
    "                               \"three_phase\" : False,\n",
    "                              #학습률 조정이 시작되는 지점을 설정합니다. 이 값은 학습률 조정이 시작되는 지점을 나타냅니다. \n",
    "                              #True로 설정하면: 학습의 마지막 단계에서 학습률을 극도로 낮게 조정하여 모델이 안정적으로 최종 가중치에 수렴할 수 있음\n",
    "                              \"pct_start\":0.1, # 기본값 0.3으로 전체 학습 중 학습률이 증가하는 단계가 어느 시점에서 시작할지를 설정합니다. 0.1로 설정되면, 학습의 10%에서 학습률이 최대에 도달하게 됩\n",
    "                              #전체 학습 에포크 수를 설정합니다. max_epochs의 값을 사용\n",
    "                              \"epochs\": self.max_epochs}\n",
    "        '''\n",
    "        학습률 스케줄링의 변화 예시\n",
    "        **max_lr=1e-2, div_factor=20, final_div_factor=1e3로 설정하면:\n",
    "        초기 학습률이 1e-2 / 20 = 5e-4로 매우 낮고, 학습 후반부에서는 1e-2 / 1e3 = 1e-5로 수렴하게 됩니다. 이는 매우 천천히 학습이 이루어지는 방식이며, 모델이 안정적으로 수렴할 가능성이 높습니다.\n",
    "        **max_lr=1e-1, div_factor=5, final_div_factor=1e2로 설정하면:\n",
    "      초기 학습률이 2e-2로 상당히 크고, 최종 학습률도 1e-3로 높습니다. 이는 빠른 학습을 목표로 하며, 불안정할 수 있지만 최적화 속도는 빠를 수 있습니다.\n",
    "        '''\n",
    "        \n",
    "        self.eval_on_train = False # 과적합을 모니터링하기 위해 훈련 세트에서 정확한 메트릭을 계산하고 싶을 수 있음\n",
    "        self.batch_size = 1 # 작게 시작합시다\n",
    "        self.gradient_accumulation = 16 // self.batch_size # 낮은 배치 크기로 훈련하지만 몇 개의 샘플에서 그래디언트를 계산할 수 있음\n",
    "        self.mixed_precision = True\n",
    "        self.num_workers = 2 # 데이터 로더에서 병렬 처리를 위해 2개의 작업자 사용\n",
    "        self.pin_memory = True # GPU에 데이터를 미리 로드하여 데이터 전송 속도를 높임\n",
    "        self.clip_value = 10.0\n",
    "\n",
    "        # 로그와 관련된 매개변수\n",
    "        self.verbose = 1 # 훈련 도중 얼마나 자주 로그를 출력할 것인지. 0일 경우 로그 출력 안함\n",
    "        self.save_path = \"working/\" # 훈련 중 저장할 \n",
    "\n",
    "# 학습과 테스트를 위한 데이터셋 경로 설정\n",
    "PATH_TO_DATA = \"input/learning-agency-lab-automated-essay-scoring-2\"\n",
    "exp_config = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91514cbd",
   "metadata": {
    "papermill": {
     "duration": 0.007187,
     "end_time": "2024-04-23T16:17:32.035341",
     "exception": false,
     "start_time": "2024-04-23T16:17:32.028154",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Define datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbd2b17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-23T16:17:32.051739Z",
     "iopub.status.busy": "2024-04-23T16:17:32.051375Z",
     "iopub.status.idle": "2024-04-23T16:17:46.134985Z",
     "shell.execute_reply": "2024-04-23T16:17:46.133946Z"
    },
    "papermill": {
     "duration": 14.094982,
     "end_time": "2024-04-23T16:17:46.137632",
     "exception": false,
     "start_time": "2024-04-23T16:17:32.042650",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from typing import Optional, Union, Any\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "def define_tokenizer(cfg):\n",
    "    \"\"\"\n",
    "    기본 AutoTokenizer 사용\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(cfg.architecture[\"backbone\"])    \n",
    "    return tokenizer\n",
    "    \n",
    "class LALDataset(Dataset):\n",
    "    \"\"\"\n",
    "    LALDataset 클래스: PyTorch Dataset을 상속받아 데이터를 모델에 맞게 준비하는 클래스를 정의합니다. \n",
    "    각 데이터셋의 정의는 학습 및 추론에서 어떻게 데이터를 사용해야 할지 결정하는 중요한 부분입니다.\n",
    "    \"\"\"\n",
    "    def __init__(self, df, config, inference, remove=True):\n",
    "        \"\"\"\n",
    "        df: pandas 데이터프레임\n",
    "        config: 학습 및 토큰화 관련된 설정 값\n",
    "        inference (bool): 추론 모드인지 여부를 나타냄\n",
    "        remove (bool): 학습에 필요하지 않은 열을 제거할지 여부를 결정\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        # tokenizer는 datacollator에서 사용되므로 정의되어야 함\n",
    "        self.tokenizer = define_tokenizer(config)\n",
    "        self.inference = inference\n",
    "        self.config = config\n",
    "        self.remove = remove\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        데이터셋의 전체 크기를 반환\n",
    "        \"\"\"\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, row_idx):\n",
    "        \n",
    "        full_text = self.df.loc[row_idx, \"full_text\"]\n",
    "        # self.tokenizer()를 사용해 해당 텍스트를 토큰화하고, 주어진 max_length와 truncation 설정에 맞춰 조정\n",
    "        tokenized_text = self.tokenizer(full_text,\n",
    "                                        return_offsets_mapping=False, # 주로 엔티티 인식을 위해 필요\n",
    "                                        truncation=self.config.token_info[\"truncation\"],\n",
    "                                        max_length=self.config.token_info[\"max_length\"])\n",
    "        \n",
    "        labels = self.df.loc[row_idx, \"score\"]\n",
    "        # df에서 해당 행의 점수(레이블)를 가져와 labels로 저장\n",
    "        # 여기서 eos 토큰을 끝에 추가하여 CLS 토큰으로 작동하게 함\n",
    "        # 모델이 인과적 Attention을 가지는 GPT 계열 모델이므로, 입력 텍스트의 마지막에 eos 토큰을 추가하여 CLS 역할을 하게 합니다\n",
    "        tokenized_text.input_ids.append(self.tokenizer.eos_token_id)\n",
    "        tokenized_text.attention_mask.append(1)\n",
    "        \"\"\"\n",
    "        토큰화: 입력 문장이 주어지면 토크나이저를 사용해 input_ids 리스트로 변환됩니다. 예를 들어, \"Hello, world!\"가 [100, 200, 300]으로 변환됩니다.\n",
    "        EOS 토큰 추가: 토크나이저에서 미리 정의된 EOS 토큰(eos_token_id)을 input_ids 리스트에 추가합니다. 예를 들어, [100, 200, 300] -> [100, 200, 300, 500].\n",
    "        Attention Mask 확장: attention_mask는 각 토큰에 대해 1로 설정하여 모델이 모든 토큰을 학습하도록 합니다. 여기서 EOS 토큰을 추가했으므로, \n",
    "        이에 대한 Attention Mask도 1을 추가하여 [1, 1, 1] -> [1, 1, 1, 1]로 확장됩니다.\n",
    "        \"\"\"\n",
    "\n",
    "        out_dict = {\n",
    "                \"input_ids\": tokenized_text.input_ids,\n",
    "                \"attention_mask\": tokenized_text.attention_mask,\n",
    "                \"labels\": torch.Tensor([labels])\n",
    "            }\n",
    "        return out_dict\n",
    "\n",
    "\n",
    "def define_loader(dataset, config, inference):\n",
    "    \"\"\"\n",
    "    훈련 및 테스트를 위한 dataloader 생성\n",
    "    \"\"\"\n",
    "    num_workers = config.num_workers\n",
    "    pin_memory = config.pin_memory\n",
    "    \n",
    "    # collate_fn = None\n",
    "    # data collator를 사용해 토큰화된 데이터를 패딩하는 방법을 사용\n",
    "    \"\"\"\n",
    "    DataCollatorWithPadding는 토큰화된 데이터를 동일한 길이로 맞춰주는 역할을 합니다. \n",
    "    이는 특히 배치(batch) 처리를 할 때 중요합니다. \n",
    "    왜냐하면, NLP 모델의 입력은 일반적으로 고정된 길이여야 하고, \n",
    "    텍스트 데이터는 길이가 다양하기 때문에 짧은 텍스트들은 패딩을 통해 길이를 맞춰줘야 합니다\n",
    "    \"\"\"\n",
    "    collate_fn = DataCollatorWithPadding(tokenizer=dataset.tokenizer,\n",
    "                                         padding=config.token_info[\"padding\"],\n",
    "                                         max_length=config.token_info[\"max_length\"],\n",
    "                                         pad_to_multiple_of=config.token_info[\"pad_to_multiple_of\"]\n",
    "                                    )\n",
    "    #padding이 True일 경우 모든 배치에서 가장 긴 문장 또는 옵션에 의해 결정되나\n",
    "    #longest는 배치 내에서 긴 문장에 따라 적용되어 각 문장에 불필요한 패딩을 최소화 할 수 있음\n",
    "\n",
    "    # Dataset을 받아 DataLoader로 변환\n",
    "    loader = DataLoader(\n",
    "                dataset,\n",
    "                batch_size=config.batch_size,\n",
    "                shuffle=not inference,\n",
    "                drop_last=not inference,\n",
    "                # 데이터 로드를 위한 병렬 처리 워커 수\n",
    "                num_workers=num_workers,\n",
    "                # GPU에 데이터를 미리 로드하여 데이터 전송 속도를 높임\n",
    "                pin_memory=pin_memory, \n",
    "                collate_fn=collate_fn,\n",
    "                # worker_init_fn=worker_init_fn,\n",
    "            )\n",
    "    return loader\n",
    "\n",
    "\n",
    "def get_dataset_and_loader(df, config, inference, remove=True):\n",
    "    \"\"\"\n",
    "    LALDataset 객체 생성: 주어진 데이터프레임과 설정을 사용해 데이터셋을 만듭니다.\n",
    "    DataLoader 생성: 만들어진 데이터셋을 기반으로 데이터 로더를 반환합니다. \n",
    "    학습 및 추론에서 사용되는 데이터셋과 데이터 로더를 모두 반환\n",
    "    \"\"\"\n",
    "    dataset = LALDataset(df, config, inference, remove=remove)\n",
    "    loader = define_loader(dataset, config, inference)\n",
    "    return dataset, loader\n",
    "\n",
    "def create_loaders(df, train_idx, valid_idx, config, eval_on_train):\n",
    "    \"\"\"\n",
    "    주어진 학습 및 검증 데이터 인덱스(train_idx, valid_idx)에 따라 학습과 검증용 데이터셋을 나누고, 각각의 데이터 로더를 생성합니다.\n",
    "    eval_on_train 옵션**: 만약 eval_on_train이 True일 경우, 학습 데이터도 검증용으로 사용할 수 있도록 추가적으로 검증용 데이터 로더(train_aux_dl)를 생성합니다.\n",
    "    최종적으로 학습용(train_dl)과 검증용(valid_dl) 데이터 로더를 반환하고, 평가할 데이터 로더와 이름(eval_loaders, eval_names)도 함께 반환합니다.\n",
    "    \"\"\"\n",
    "    # 추론을 위해 더 큰 max length 설정 가능\n",
    "    valid_config = copy.deepcopy(config)\n",
    "    valid_config.token_info['max_length'] = config.token_info['max_length']\n",
    "    \n",
    "    _, train_dl = get_dataset_and_loader(df=df.iloc[train_idx].reset_index(drop=True),\n",
    "                                        config=config,\n",
    "                                        inference=False,\n",
    "                                        )\n",
    "\n",
    "    _, valid_dl = get_dataset_and_loader(df=df.iloc[valid_idx].reset_index(drop=True),\n",
    "                                        config=valid_config,\n",
    "                                        inference=True)\n",
    "\n",
    "    if eval_on_train:\n",
    "        _, train_aux_dl = get_dataset_and_loader(df=df.iloc[train_idx].reset_index(drop=True),\n",
    "                                                config=valid_config,\n",
    "                                                inference=True)\n",
    "\n",
    "        eval_loaders = [train_aux_dl, valid_dl]\n",
    "        eval_names = [\"train\", \"valid\"]\n",
    "    else:\n",
    "        eval_loaders = [valid_dl]\n",
    "        eval_names = [\"valid\"]\n",
    "    return train_dl, valid_dl, eval_loaders, eval_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83c5c8b",
   "metadata": {
    "papermill": {
     "duration": 0.006972,
     "end_time": "2024-04-23T16:17:46.152077",
     "exception": false,
     "start_time": "2024-04-23T16:17:46.145105",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 네트워크의 아키텍처 정의\n",
    "\n",
    "여기서는 LORA로 미세 조정된 LLM backbone과 linear head로 구성된 간단한 아키텍처를 사용합니다.\n",
    "우리는 최종 점수 예측을 위해 마지막 eos_token만 사용합니다.\n",
    "\n",
    "torch.nn.Module을 사용하여 이 아키텍처를 쉽게 사용자 정의할 수 있습니다:\n",
    "- tf-idf, num_words, engineered features 등과 같은 메타데이터를 입력으로 추가\n",
    "- 최종 head를 더 복잡하게 만들기 (MLP with activations 등)\n",
    "- eos_token pooling 대신 pooling methods 시도 등"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46ced460",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-23T16:17:46.168981Z",
     "iopub.status.busy": "2024-04-23T16:17:46.168351Z",
     "iopub.status.idle": "2024-04-23T16:17:47.329949Z",
     "shell.execute_reply": "2024-04-23T16:17:47.328872Z"
    },
    "papermill": {
     "duration": 1.17361,
     "end_time": "2024-04-23T16:17:47.332993",
     "exception": false,
     "start_time": "2024-04-23T16:17:46.159383",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoModelForCausalLM\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from transformers import AutoConfig\n",
    "\n",
    "class CustomLLM(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    사용자 정의 아키텍처를 설정할 수 있는 클래스입니다.\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg, eos_token_id):\n",
    "        super().__init__()\n",
    "        self.num_classes = cfg.num_classes  # 예측할 클래스의 수\n",
    "        self.eos_token_id = eos_token_id  # 문장의 끝을 나타내는 토큰 ID\n",
    "        self.model_config = AutoConfig.from_pretrained(\n",
    "                cfg.architecture[\"backbone\"],\n",
    "            )  # 사전 학습된 모델의 구성 불러오기\n",
    "\n",
    "        self.activation = torch.nn.Identity()  # 기본 활성화 함수 설정\n",
    "        # 네트워크의 backbone 설정\n",
    "        self.backbone  = AutoModelForCausalLM.from_pretrained(cfg.architecture[\"backbone\"],\n",
    "                                                                    device_map=\"cpu\",  # 모델을 로드할 장치\n",
    "                                                                    load_in_4bit=False,  # 4비트 로드 설정\n",
    "                                                                    torch_dtype=torch.float32,  # 데이터 타입 설정\n",
    "                                                                    **cfg.architecture[\"params\"])\n",
    "        # 사용자 정의 head를 사용하기 위해 기존 head 제거\n",
    "        self.backbone.lm_head = torch.nn.Identity()\n",
    "\n",
    "        if cfg.remove_layers is not None:\n",
    "            # 마지막 레이어 제거: 불필요한 레이어 제거\n",
    "            self.backbone.layers = self.backbone.model.layers[:-cfg.remove_layers]          \n",
    "        \n",
    "        if hasattr(cfg, \"num_layers_to_freeze\"):\n",
    "            print(f\"freezing {cfg.num_layers_to_freez} layers.\")\n",
    "            if cfg.num_layers_to_freeze > 0:\n",
    "                if cfg.freeze_embeddings:\n",
    "                    # 임베딩 레이어 고정\n",
    "                    for param in self.backbone.embed_tokens.parameters():\n",
    "                        param.requires_grad = False\n",
    "                # 첫 번째 레이어 고정: 나머지 마지막 레이어만 훈련\n",
    "                for layer in self.backbone.model.layers[:cfg.num_layers_to_freeze]:\n",
    "                    for param in layer.parameters():\n",
    "                        param.requires_grad = False\n",
    "                \n",
    "        if cfg.use_lora:\n",
    "            # peft 라이브러리에서 lora 적용\n",
    "            peft_config = LoraConfig(\n",
    "                task_type=TaskType.CAUSAL_LM,\n",
    "                inference_mode=False,  # 추론 모드 설정\n",
    "                r=cfg.lora_config[\"r\"],\n",
    "                lora_alpha=cfg.lora_config[\"lora_alpha\"],\n",
    "                lora_dropout=cfg.lora_config[\"lora_dropout\"],\n",
    "                target_modules=cfg.lora_config[\"target_modules\"],\n",
    "            )\n",
    "            \n",
    "            self.backbone = get_peft_model(self.backbone, peft_config)\n",
    "        else:\n",
    "            print(\"NOT USING LORA\")\n",
    "            \n",
    "        # gradient checkpoint는 나중에 사용\n",
    "        # self.transformers_model.gradient_checkpointing_enable()\n",
    "                    \n",
    "        self.final_linear = torch.nn.Linear(self.model_config.hidden_size, cfg.num_classes)  # 최종 예측을 위한 선형 레이어\n",
    "        \n",
    "        if cfg.use_lora:\n",
    "            self.backbone.print_trainable_parameters()  # 훈련 가능한 파라미터 출력\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        x = batch[\"input_ids\"]  # 입력 데이터의 ID\n",
    "        # 각 예제에 하나의 eos_token만 있다고 가정\n",
    "        eos_positions = torch.argwhere(x == self.eos_token_id)[:, 1]\n",
    "        x = self.backbone(\n",
    "            input_ids=x,\n",
    "            attention_mask=batch[\"attention_mask\"],\n",
    "        )[\"logits\"]  # 모델의 출력 logits\n",
    "        \n",
    "        # eos_token에 해당하는 위치의 logits만 사용\n",
    "        x = x[torch.arange(x.shape[0]), eos_positions]  # (bs, hidden_size)\n",
    "        \n",
    "        logits = self.final_linear(x)  # 최종 예측 생성\n",
    "\n",
    "        return {\"logits\": logits}  # 예측 결과 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92136cd9",
   "metadata": {
    "papermill": {
     "duration": 0.009288,
     "end_time": "2024-04-23T16:17:47.353206",
     "exception": false,
     "start_time": "2024-04-23T16:17:47.343918",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Training recipe\n",
    "\n",
    "You may need to change this if you make significant changes in your modelling apporach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45523b46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-23T16:17:47.370051Z",
     "iopub.status.busy": "2024-04-23T16:17:47.369582Z",
     "iopub.status.idle": "2024-04-23T16:17:47.478004Z",
     "shell.execute_reply": "2024-04-23T16:17:47.476975Z"
    },
    "papermill": {
     "duration": 0.119852,
     "end_time": "2024-04-23T16:17:47.480479",
     "exception": false,
     "start_time": "2024-04-23T16:17:47.360627",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import List, Any, Dict\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from abc import abstractmethod\n",
    "from sklearn.base import BaseEstimator\n",
    "import json\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "import copy\n",
    "\n",
    "# AdamW 옵티마이저에서 weight decay를 적용하지 않을 레이어들\n",
    "ALL_LAYERNORM_LAYERS = [torch.nn.LayerNorm, torch.nn.Embedding]\n",
    "\n",
    "def get_parameter_names(network, forbidden_layer_types):\n",
    "    \"\"\"\n",
    "    모델 파라미터 중 금지된 레이어에 속하지 않는 파라미터의 이름을 반환합니다.\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for name, child in network.named_children():\n",
    "        result += [\n",
    "            f\"{name}.{n}\"\n",
    "            for n in get_parameter_names(child, forbidden_layer_types)\n",
    "            if not isinstance(child, tuple(forbidden_layer_types))\n",
    "        ]\n",
    "    # 모델에 특정된 파라미터 추가 (nn.Parameter로 정의된 것들)\n",
    "    result += list(network._parameters.keys())\n",
    "    return result\n",
    "\n",
    "def define_loss_function(loss_config):\n",
    "    \"\"\"\n",
    "    기본 torch 손실 함수 또는 로컬에서 정의된 손실 함수\n",
    "    \"\"\"\n",
    "    copy_config = copy.copy(loss_config)\n",
    "    loss_name = copy_config.pop('loss_name')\n",
    "    try:\n",
    "        loss_fn = getattr(torch.nn, loss_name)(**copy_config)\n",
    "    except AttributeError:\n",
    "        try:\n",
    "            loss_fn = globals().get(loss_name)(copy_config)\n",
    "        except:\n",
    "            raise NotImplementedError(\"알 수 없는 손실 함수 :\", loss_name)\n",
    "    return loss_fn\n",
    "\n",
    "def prepare_log_folder(log_path):\n",
    "    \"\"\"\n",
    "    실험 폴더를 생성하는 유틸리티 함수\n",
    "    로그를 저장할 디렉토리를 생성합니다.\n",
    "    로그는 log_path/date_of_day/exp_id에 저장됩니다.\n",
    "\n",
    "    Args:\n",
    "        log_path (str): 디렉토리 경로\n",
    "\n",
    "    Returns:\n",
    "        str: 생성된 로그 폴더의 경로\n",
    "    \"\"\"\n",
    "    today = str(datetime.date.today())\n",
    "    log_today = os.path.join(log_path, today)\n",
    "\n",
    "    if not os.path.exists(log_today):\n",
    "        Path(log_today).mkdir(parents=True)\n",
    "\n",
    "    exp_id = (\n",
    "        np.max([int(f) if str(f).isdigit() else -1 for f in os.listdir(log_today)]) + 1\n",
    "        if len(os.listdir(log_today))\n",
    "        else 0\n",
    "    )\n",
    "    log_folder = os.path.join(log_today, f\"{exp_id}\")\n",
    "\n",
    "    assert not os.path.exists(log_folder), \"실험이 이미 존재합니다\"\n",
    "    os.mkdir(log_folder)\n",
    "    print(\"로그를 저장합니다 :\", log_folder)\n",
    "    return log_folder\n",
    "\n",
    "def save_config(config, folder):\n",
    "    \"\"\"\n",
    "    설정을 json으로 저장하고, 데이터 및 모델 설정을 복사합니다.\n",
    "\n",
    "    Args:\n",
    "        config (Config): 설정 객체\n",
    "        folder (str): 저장할 폴더 경로\n",
    "    \"\"\"\n",
    "    with open(os.path.join(folder, \"config.json\"), \"w\") as f:\n",
    "        json.dump(config.__dict__.copy(), f)\n",
    "\n",
    "@dataclass\n",
    "class AbstractBaseModel(BaseEstimator):\n",
    "    \"\"\" scikit과 유사한 모델을 위한 추상 클래스.\n",
    "        훈련, 추론, 저장, 로드 등을 구축할 수 있습니다.\n",
    "    \"\"\"\n",
    "\n",
    "    network: torch.nn.Module = None\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    seed: int = 42\n",
    "    mixed_precision: bool = False\n",
    "\n",
    "    def __post_init__(self):\n",
    "        torch.manual_seed(self.seed)\n",
    "        self.network.to(self.device)\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        train_dataloader,\n",
    "        eval_loaders=None,\n",
    "        eval_names=None,\n",
    "        eval_metric=None,\n",
    "        loss_config=None,\n",
    "        max_epochs=100,\n",
    "        callbacks=None,\n",
    "        optimizer_name=\"Adam\",\n",
    "        optimizer_params={\"lr\": 1e-3},\n",
    "        gradient_accumulation=None,\n",
    "        scheduler_name=None,\n",
    "        scheduler_params=None,\n",
    "        mixed_precision=False,\n",
    "        clip_value=None,\n",
    "        log_path=None,\n",
    "        verbose=1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        self.network에 저장된 신경망을 훈련합니다.\n",
    "        train_dataloader를 사용하여 훈련 데이터를 사용하고\n",
    "        eval_loaders를 사용하여 검증합니다.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        train_dataloader : Dataloader\n",
    "            훈련 세트\n",
    "        eval_loader : list of dataloaders\n",
    "            마지막 하나는 조기 중단에 사용됩니다.\n",
    "        eval_name : list of str\n",
    "            평가 세트 이름 목록\n",
    "        eval_metric : list of str\n",
    "            평가 메트릭 목록\n",
    "            마지막 메트릭은 조기 중단에 사용됩니다.\n",
    "        loss_name : Name\n",
    "            PyTorch 손실 함수 이름\n",
    "        max_epochs : int\n",
    "            훈련 중 최대 에포크 수\n",
    "        num_workers : int\n",
    "            torch.utils.data.DataLoader에서 사용되는 작업자 수\n",
    "        drop_last : bool\n",
    "            훈련 중 마지막 배치를 버릴지 여부\n",
    "        pin_memory: bool\n",
    "            훈련 중 pin_memory를 True 또는 False로 설정할지 여부\n",
    "        from_unsupervised: unsupervised trained model\n",
    "            이전에 self supervised 모델을 시작 가중치로 사용\n",
    "        clip_value: float (기본값 None)\n",
    "            그래디언트 클리핑\n",
    "        \"\"\"\n",
    "        # 모델 이름 업데이트\n",
    "\n",
    "        self.max_epochs = max_epochs\n",
    "        self._stop_training = False\n",
    "        self.optimizer_name = optimizer_name\n",
    "        self.optimizer_params = optimizer_params\n",
    "        eval_loaders = eval_loaders if eval_loaders else []       \n",
    "        self.mixed_precision = mixed_precision\n",
    "        self.clip_value = clip_value\n",
    "        self.verbose = verbose\n",
    "        self.gradient_accumulation = gradient_accumulation\n",
    "        self.metrics = eval_metric\n",
    "        \n",
    "        if loss_config is None:\n",
    "            raise(NotImplementedError, \"손실을 지정하세요\")\n",
    "        else:\n",
    "            self.loss_fn = define_loss_function(loss_config)\n",
    "        \n",
    "        self._set_optimizer()\n",
    "        \n",
    "        # 스케줄러 설정\n",
    "        self.scheduler_fn = getattr(torch.optim.lr_scheduler, scheduler_name)  # torch 스케줄러만 허용\n",
    "        self.scheduler_params = copy.copy(scheduler_params)\n",
    "        self.scheduler = self.scheduler_fn(self._optimizer, **self.scheduler_params)\n",
    "        \n",
    "        # 에포크 동안 훈련 루프\n",
    "        start_time = time.time()\n",
    "        for epoch_idx in range(self.max_epochs):\n",
    "            self.epoch_idx = epoch_idx\n",
    "            epoch_loss, epoch_lr = self._train_epoch(train_dataloader)\n",
    "            msg = f\"epoch {epoch_idx:<3} | lr: {epoch_lr:.2e} | loss: {epoch_loss:.4f} \"\n",
    "            # 모든 평가 세트에 대해 예측 에포크 적용\n",
    "            if ((self.verbose != 0) and (epoch_idx % self.verbose == 0)) or (epoch_idx==self.max_epochs-1):\n",
    "                for eval_name, valid_dataloader in zip(eval_names, eval_loaders):\n",
    "                    with torch.no_grad():\n",
    "                        prob_pred, prob_true, scores = self._predict_epoch(eval_name, valid_dataloader)\n",
    "                    for metric_name, metric_score in scores:\n",
    "                        msg += f\"| {metric_name:<3} ({eval_name}): {metric_score:.4f} \"\n",
    "            total_time = int(time.time() - start_time)\n",
    "            msg += f\"|  {str(datetime.timedelta(seconds=total_time)) + 's':<6}\"\n",
    "            print(msg)\n",
    "        print(\"훈련 종료!\")\n",
    "        self.network.eval()\n",
    "        return prob_pred, prob_true\n",
    "        \n",
    "    def predict_proba(self, dataloader, return_target=False):\n",
    "        \"\"\"\n",
    "        배치에 대한 예측 수행 (유효성 검사)\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : a :tensor: `torch.Tensor`\n",
    "            입력 데이터\n",
    "        Returns\n",
    "        -------\n",
    "        predictions : np.array\n",
    "            회귀 문제의 예측\n",
    "        \"\"\"\n",
    "        self.network.eval()\n",
    "        results_prob = []\n",
    "        results_targets = []\n",
    "        pbar = tqdm(dataloader,\n",
    "                     leave=False,\n",
    "                     total=len(dataloader),\n",
    "                     desc=f'Inference')\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in pbar:\n",
    "                out_probs = self._predict_batch(batch).cpu()\n",
    "                results_prob.append(out_probs)\n",
    "\n",
    "                if return_target:\n",
    "                    targets = batch[\"labels\"]\n",
    "                    targets = targets.to(\"cpu\").detach()\n",
    "                    results_targets.append(targets)\n",
    "\n",
    "        res_prob = self.stack_preds(results_prob)                \n",
    "        if return_target:\n",
    "            res_target = self.stack_targets(results_targets)\n",
    "            return res_prob, res_target\n",
    "        else:\n",
    "            return res_prob\n",
    "\n",
    "    def save_model(self, path, model_name):\n",
    "        \"\"\"\n",
    "        모델을 저장합니다.\n",
    "\n",
    "        사용자는 경로와 모델 이름을 모두 지정할 수 있습니다.\n",
    "        모델 이름이 주어지지 않으면 자동으로 생성됩니다.\n",
    "        \"\"\"\n",
    "        Path(path).mkdir(parents=True, exist_ok=True)\n",
    "        # 추론 중 GPU 사용량을 줄이기 위해 절반 정밀도로 state_dict 저장: 혼합 정밀도로 훈련된 경우 동일한 결과\n",
    "        torch.save(self.network.half().state_dict(), Path(path).joinpath(f\"{model_name}.pt\"))\n",
    "        # torch.save(self.network.state_dict(), Path(path).joinpath(f\"{model_name}.pt\"))\n",
    "        return\n",
    "\n",
    "    def _train_epoch(self, train_loader):\n",
    "        \"\"\"\n",
    "        self.network에서 네트워크의 한 에포크를 훈련합니다.\n",
    "        Parameters\n",
    "        ----------\n",
    "        train_loader : a :class: `torch.utils.data.Dataloader`\n",
    "            훈련 세트가 있는 DataLoader\n",
    "        \"\"\"\n",
    "        self.network.train()\n",
    "        num_iter_epoch = len(train_loader)\n",
    "        pbar = tqdm(enumerate(train_loader),\n",
    "                                     leave=False,\n",
    "                                     total=len(train_loader),\n",
    "                                     desc=f'train epoch {self.epoch_idx}')\n",
    "        \n",
    "        epoch_loss = 0\n",
    "        for batch_idx, batch in pbar:\n",
    "            batch_loss = self._train_batch(batch, batch_idx, num_iter_epoch)\n",
    "            epoch_loss = (train_loader.batch_size*batch_idx*epoch_loss + train_loader.batch_size*batch_loss) / (train_loader.batch_size*(batch_idx+1))            \n",
    "            pbar.set_description(f'train epoch {self.epoch_idx}: loss {epoch_loss:.3f}', refresh=True)\n",
    "            # 스케줄러 업데이트\n",
    "            self.scheduler.step()\n",
    "\n",
    "        epoch_lr = self._optimizer.param_groups[-1][\"lr\"]\n",
    "        return epoch_loss, epoch_lr\n",
    "\n",
    "    def _train_batch(self, batch, batch_idx, num_iter_epoch):\n",
    "        \"\"\"\n",
    "        데이터의 한 배치를 훈련합니다.\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_logs : dict\n",
    "            \"batch_size\"와 \"loss\"가 있는 사전\n",
    "        \"\"\"\n",
    "        self._send_batch_to_device(batch)\n",
    "                                   \n",
    "        with torch.cuda.amp.autocast(enabled=self.mixed_precision):\n",
    "            # float16 훈련을 위한 혼합 정밀도 사용\n",
    "            y = batch[\"labels\"]\n",
    "            batch_logs = {\"batch_size\": y.shape[0]}\n",
    "            out_probs = self.network(batch)\n",
    "            # 그래디언트 누적에 의한 손실 계산\n",
    "            loss = self.loss_fn(out_probs[\"logits\"], y.unsqueeze(-1)) / self.gradient_accumulation\n",
    "            self.scaler.scale(loss).backward()\n",
    "            \n",
    "        if ((batch_idx + 1) % self.gradient_accumulation == 0) or ((batch_idx + 1)==num_iter_epoch):\n",
    "            # 역전파 및 최적화 수행\n",
    "            if self.clip_value is not None:\n",
    "                self.scaler.unscale_(self._optimizer)\n",
    "                clip_grad_norm_(self.network.parameters(), max_norm=self.clip_value)\n",
    "\n",
    "            self.scaler.step(self._optimizer)\n",
    "            self.scaler.update()\n",
    "            # step을 호출할 때만 그래디언트를 0으로 설정\n",
    "            self._optimizer.zero_grad(set_to_none=True)\n",
    "        return loss.detach().item()\n",
    "\n",
    "    def _predict_epoch(self, name, loader):\n",
    "        \"\"\"\n",
    "        에포크를 예측하고 메트릭을 업데이트합니다.\n",
    "        Parameters\n",
    "        ----------\n",
    "        name : str\n",
    "            검증 세트의 이름\n",
    "        loader : torch.utils.data.Dataloader\n",
    "                검증 세트가 있는 DataLoader\n",
    "        \"\"\"\n",
    "        prob_pred, prob_true = self.predict_proba(loader, return_target=True)\n",
    "        \n",
    "        scores = []\n",
    "        for metric_fn in self.metrics:\n",
    "            metric_score = metric_fn(prob_true, prob_pred)\n",
    "            scores.append((metric_fn._name, metric_score))\n",
    "        # 여기서 메트릭을 계산해야 함\n",
    "        return prob_pred, prob_true, scores\n",
    "\n",
    "    def stack_preds(self, list_prob):\n",
    "        return torch.vstack(list_prob)\n",
    "\n",
    "    def stack_targets(self, list_prob):\n",
    "        return torch.hstack(list_prob)\n",
    "\n",
    "    def _send_batch_to_device(self, batch):\n",
    "        for key, value in batch.items():\n",
    "            batch[key] = value.to(self.device)\n",
    "            \n",
    "    def _predict_batch(self, batch):\n",
    "        \"\"\"\n",
    "        데이터의 한 배치를 예측합니다.\n",
    "        \"\"\"\n",
    "        with torch.cuda.amp.autocast(enabled=self.mixed_precision):\n",
    "            self._send_batch_to_device(batch)\n",
    "            # 모델 출력 계산\n",
    "            out_probs = self.network(batch)[\"logits\"]\n",
    "            # 활성화 적용\n",
    "            if isinstance(self.network, torch.nn.DataParallel):\n",
    "                # 데이터 병렬 처리\n",
    "                out_probs = self.network.module.activation(out_probs)\n",
    "            else:\n",
    "                out_probs = self.network.activation(out_probs)\n",
    "            \n",
    "        return out_probs.detach()\n",
    "\n",
    "    def _set_optimizer(self):\n",
    "        \"\"\"옵티마이저 설정.\"\"\"\n",
    "        \n",
    "        name = self.optimizer_name\n",
    "\n",
    "        # 레이어 정규화에 대해 decay 비활성화\n",
    "        decay_parameters = get_parameter_names(self.network, ALL_LAYERNORM_LAYERS)\n",
    "        decay_parameters = [name for name in decay_parameters if \"bias\" not in name]\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p for n, p in self.network.named_parameters() if (n in decay_parameters and p.requires_grad)\n",
    "                ],\n",
    "                \"weight_decay\": self.optimizer_params[\"weight_decay\"],\n",
    "            },\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p for n, p in self.network.named_parameters() if (n not in decay_parameters and p.requires_grad)\n",
    "                ],\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "        other_params = self.optimizer_params.copy()\n",
    "        _ = other_params.pop(\"weight_decay\")\n",
    "                \n",
    "        self._optimizer = getattr(torch.optim, name)(optimizer_grouped_parameters, **other_params)        \n",
    "        self.scaler = torch.cuda.amp.GradScaler(enabled=self.mixed_precision)\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6616ed29",
   "metadata": {
    "papermill": {
     "duration": 0.008329,
     "end_time": "2024-04-23T16:17:47.513668",
     "exception": false,
     "start_time": "2024-04-23T16:17:47.505339",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Metrics to track\n",
    "\n",
    "Here you can define metrics you want to track during model training (every epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e39a0d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-23T16:17:47.534260Z",
     "iopub.status.busy": "2024-04-23T16:17:47.533337Z",
     "iopub.status.idle": "2024-04-23T16:17:48.049346Z",
     "shell.execute_reply": "2024-04-23T16:17:48.048540Z"
    },
    "papermill": {
     "duration": 0.527751,
     "end_time": "2024-04-23T16:17:48.051734",
     "exception": false,
     "start_time": "2024-04-23T16:17:47.523983",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    mean_squared_error\n",
    ")\n",
    "\n",
    "class RMSE:\n",
    "    \"\"\"\n",
    "    Root Mean Squared Error (RMSE)를 계산하는 클래스입니다.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._name = \"rmse\"  # 메트릭의 이름을 설정합니다.\n",
    "\n",
    "    def __call__(self, y_true, y_score):\n",
    "        \"\"\"\n",
    "        예측의 평균 제곱 오차(MSE)를 계산합니다.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_true : np.ndarray\n",
    "            실제 값의 행렬 또는 벡터\n",
    "        y_score : np.ndarray\n",
    "            예측 값의 행렬 또는 벡터\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            예측과 실제 값 간의 MSE.\n",
    "        \"\"\"\n",
    "        # squared=False로 설정하여 RMSE를 반환합니다.\n",
    "        return mean_squared_error(y_true.numpy(), y_score.numpy(), squared=False)\n",
    "\n",
    "import numpy as np\n",
    "from numba import jit \n",
    "\n",
    "# @jit\n",
    "def qwk6(a1, a2, max_rat=6):\n",
    "    \"\"\"\n",
    "    CPMP에서 적응된 경쟁 메트릭: https://www.kaggle.com/c/prostate-cancer-grade-assessment/discussion/145105\n",
    "    \"\"\"\n",
    "    assert(len(a1) == len(a2))  # 두 배열의 길이가 같아야 합니다.\n",
    "    \n",
    "    a1 = a1.astype(np.int64).reshape(-1)  # a1을 정수형으로 변환하고 1차원으로 변형합니다.\n",
    "    # 연속적인 예측을 가장 가까운 정수로 변환합니다.\n",
    "    a2 = np.rint(a2).astype(np.int64).reshape(-1)\n",
    "\n",
    "    hist1 = np.zeros((max_rat + 1, ))  # a1의 히스토그램을 저장할 배열\n",
    "    hist2 = np.zeros((max_rat + 1, ))  # a2의 히스토그램을 저장할 배열\n",
    "\n",
    "    o = 0  # 관측된 차이의 제곱합\n",
    "    for k in range(a1.shape[0]):\n",
    "        i, j = a1[k], a2[k]\n",
    "        hist1[i] += 1\n",
    "        hist2[j] += 1\n",
    "        o +=  (i - j) * (i - j)\n",
    "\n",
    "    e = 0  # 기대 차이의 제곱합\n",
    "    for i in range(max_rat + 1):\n",
    "        for j in range(max_rat + 1):\n",
    "            e += hist1[i] * hist2[j] * (i - j) * (i - j)\n",
    "\n",
    "    e = e / a1.shape[0]  # 기대 차이의 평균\n",
    "    return (1 - o / e)  # QWK 점수 반환\n",
    "\n",
    "class QWK:\n",
    "    def __init__(self):\n",
    "        self._name = \"qwk\"  # 메트릭의 이름을 설정합니다.\n",
    "    def __call__(self, y_true, y_pred, max_rat=6):\n",
    "        # QWK 점수를 계산합니다.\n",
    "        return qwk6(y_true.numpy(), y_pred.numpy())\n",
    "\n",
    "# from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "# class ScikitQWK:\n",
    "#     \"\"\"\n",
    "#     scikit을 사용한 경쟁 메트릭\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self):\n",
    "#         self._name = \"scikit_qwk\"  # 메트릭의 이름을 설정합니다.\n",
    "#     def __call__(self, y_true, y_pred):\n",
    "#         y_pred = np.rint(y_pred)  # 예측을 가장 가까운 정수로 변환합니다.\n",
    "#         return cohen_kappa_score(y_true, y_pred, weights=\"quadratic\")  # QWK 점수 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2447f8cd",
   "metadata": {
    "papermill": {
     "duration": 0.007908,
     "end_time": "2024-04-23T16:17:48.067082",
     "exception": false,
     "start_time": "2024-04-23T16:17:48.059174",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Puting everything together for training one fold\n",
    "\n",
    "This is just a simple function that will allow you to train one fold and save the corresponding configs and model checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75664778",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-23T16:17:48.083385Z",
     "iopub.status.busy": "2024-04-23T16:17:48.083013Z",
     "iopub.status.idle": "2024-04-23T16:17:48.097605Z",
     "shell.execute_reply": "2024-04-23T16:17:48.096703Z"
    },
    "papermill": {
     "duration": 0.02606,
     "end_time": "2024-04-23T16:17:48.099962",
     "exception": false,
     "start_time": "2024-04-23T16:17:48.073902",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def update_sched_params(config, train_loader):\n",
    "    \"\"\"\n",
    "    이 헬퍼 함수는 에포크당 단계 수를 동적으로 정의할 수 있게 합니다.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    - config : 실험 설정\n",
    "    - train_loader : 이 폴드에 사용되는 훈련 데이터 로더\n",
    "    \"\"\"\n",
    "    nb_epochs = config.max_epochs  # 최대 에포크 수\n",
    "    is_per_epoch = config.scheduler_params.get(\"steps_per_epoch\", None)\n",
    "\n",
    "    if is_per_epoch is not None:\n",
    "        if is_per_epoch <= 0:\n",
    "            # 자동으로 단계 수를 설정\n",
    "            config.scheduler_params[\"steps_per_epoch\"] = len(train_loader)\n",
    "        # 그렇지 않으면 정의된 값을 사용\n",
    "    \n",
    "    # get_cosine_schedule_with_warmup을 위한 설정\n",
    "    warmup_ratio = config.scheduler_params.pop(\"warmup_ratio\", None)\n",
    "\n",
    "    if warmup_ratio is not None:\n",
    "        num_train_steps = int(len(train_loader) * nb_epochs)  # 총 훈련 단계 수\n",
    "        num_warmup_steps = int(num_train_steps * warmup_ratio)  # 워밍업 단계 수\n",
    "        config.scheduler_params[\"num_warmup_steps\"] = num_warmup_steps\n",
    "        config.scheduler_params[\"num_training_steps\"] = num_train_steps\n",
    "        # 그렇지 않으면 정의된 값을 사용\n",
    "    return config\n",
    "\n",
    "def train_fold(df,\n",
    "               train_idx,\n",
    "               valid_idx,\n",
    "               config,\n",
    "               fold_nb):\n",
    "    \"\"\"\n",
    "    주어진 데이터 프레임과 인덱스를 사용하여 하나의 폴드를 훈련합니다.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    - df : 데이터 프레임\n",
    "    - train_idx : 훈련 데이터 인덱스\n",
    "    - valid_idx : 검증 데이터 인덱스\n",
    "    - config : 실험 설정\n",
    "    - fold_nb : 폴드 번호\n",
    "    \"\"\"\n",
    "    print(\"Num train and valid samples:\", train_idx.shape[0], valid_idx.shape[0])\n",
    "    config = copy.deepcopy(config)  # 설정을 깊은 복사하여 수정\n",
    "    train_dl, valid_dl, eval_loaders, eval_names = create_loaders(df,\n",
    "                                                                  train_idx,\n",
    "                                                                  valid_idx,\n",
    "                                                                  config,\n",
    "                                                                  eval_on_train=config.eval_on_train\n",
    "                                                                  )\n",
    "    log_folder = prepare_log_folder(config.save_path)  # 로그 폴더 준비\n",
    "    # eos_token_id를 설정에 추가\n",
    "    config.eos_token_id = train_dl.dataset.tokenizer.eos_token_id\n",
    "    save_config(config, log_folder)  # 설정을 저장\n",
    "\n",
    "    # 네트워크와 모델 초기화\n",
    "    network = CustomLLM(config, train_dl.dataset.tokenizer.eos_token_id)\n",
    "    model = AbstractBaseModel(network=network)\n",
    "        \n",
    "    # 스케줄러 업데이트\n",
    "    config = update_sched_params(config, train_dl)\n",
    "\n",
    "    # 모델 훈련\n",
    "    prob_pred, prob_true = model.fit(train_dl,\n",
    "                                      eval_loaders=eval_loaders,\n",
    "                                      eval_names=eval_names,\n",
    "                                      eval_metric=[RMSE(), QWK()],  # 평가 메트릭 설정\n",
    "                                      loss_config=config.loss_config, \n",
    "                                      max_epochs=config.max_epochs,\n",
    "                                      callbacks=None,\n",
    "                                      optimizer_name=config.optimizer_name,\n",
    "                                      optimizer_params=config.optimizer_params,\n",
    "                                      scheduler_name=config.scheduler_name,\n",
    "                                      scheduler_params=config.scheduler_params,\n",
    "                                      gradient_accumulation=config.gradient_accumulation,\n",
    "                                      mixed_precision=config.mixed_precision,\n",
    "                                      clip_value=config.clip_value,\n",
    "                                      verbose=config.verbose,\n",
    "             )\n",
    "\n",
    "    # 모델 저장\n",
    "    model.save_model(path=log_folder, model_name=f\"fold_{fold_nb}\")\n",
    "    torch.cuda.empty_cache()  # 캐시 비우기\n",
    "        \n",
    "    return prob_pred, prob_true  # 예측 결과 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cd3f79",
   "metadata": {
    "papermill": {
     "duration": 0.006871,
     "end_time": "2024-04-23T16:17:48.114085",
     "exception": false,
     "start_time": "2024-04-23T16:17:48.107214",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Training: 5 fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af5d475",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-23T16:17:48.129630Z",
     "iopub.status.busy": "2024-04-23T16:17:48.129262Z",
     "iopub.status.idle": "2024-04-23T16:17:48.990492Z",
     "shell.execute_reply": "2024-04-23T16:17:48.989521Z"
    },
    "papermill": {
     "duration": 0.872338,
     "end_time": "2024-04-23T16:17:48.993402",
     "exception": false,
     "start_time": "2024-04-23T16:17:48.121064",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Stratified K-Fold를 사용하여 데이터셋을 나눕니다.\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import os\n",
    "\n",
    "# 훈련 데이터를 다운로드합니다.\n",
    "df_train = pd.read_csv(os.path.join(PATH_TO_DATA, \"train.csv\"))\n",
    "\n",
    "# 훈련 및 추론 모드 설정\n",
    "TRAIN = False  # 훈련을 위해 True로 설정하고, 추론을 위해 False로 설정\n",
    "INFERENCE = True\n",
    "DEBUG = False\n",
    "\n",
    "if TRAIN:\n",
    "    if DEBUG:\n",
    "        df_train = df_train[:50]  # 디버그 모드에서는 데이터의 일부만 사용\n",
    "    # Stratified K-Fold를 사용하여 데이터를 5개의 폴드로 나눕니다.\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    # 각 폴드에 대해 훈련 및 검증 인덱스를 생성합니다.\n",
    "    for fold_nb, (train_idx, valid_idx) in enumerate(skf.split(df_train, df_train.score)):\n",
    "        # 각 폴드에 대해 모델을 훈련합니다.\n",
    "        prob_pred, prob_true = train_fold(df_train,\n",
    "                                           train_idx,\n",
    "                                           valid_idx,\n",
    "                                           exp_config,\n",
    "                                           fold_nb=fold_nb,\n",
    "                                           )\n",
    "        break  # 첫 번째 폴드만 훈련하고 종료"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef96e76c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-23T16:17:49.011943Z",
     "iopub.status.busy": "2024-04-23T16:17:49.011568Z",
     "iopub.status.idle": "2024-04-23T16:17:49.016541Z",
     "shell.execute_reply": "2024-04-23T16:17:49.015438Z"
    },
    "papermill": {
     "duration": 0.016588,
     "end_time": "2024-04-23T16:17:49.018883",
     "exception": false,
     "start_time": "2024-04-23T16:17:49.002295",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example of what you should see (trained on my personal setup)\n",
    "\n",
    "# Num train and valid samples: 13845 3462\n",
    "# Saving logs at : ../../logs/essay_scoring/2024-04-23/2\n",
    "# trainable params: 9,805,824 || all params: 2,515,978,240 || trainable%: 0.3897420034920493\n",
    "# epoch 0   | lr: 5.87e-05 | loss: 0.0662 | rmse (valid): 0.5429 | qwk (valid): 0.8105 |  0:45:41s\n",
    "# epoch 1   | lr: 1.00e-07 | loss: 0.0301 | rmse (valid): 0.5167 | qwk (valid): 0.8358 |  1:31:20s\n",
    "# End of training!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcaa1b0",
   "metadata": {
    "papermill": {
     "duration": 0.007225,
     "end_time": "2024-04-23T16:17:49.033879",
     "exception": false,
     "start_time": "2024-04-23T16:17:49.026654",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Inference\n",
    "\n",
    "Here is where you can perform simple inference from a previously trained checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df242f14",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-23T16:17:49.050653Z",
     "iopub.status.busy": "2024-04-23T16:17:49.050258Z",
     "iopub.status.idle": "2024-04-23T16:17:49.059666Z",
     "shell.execute_reply": "2024-04-23T16:17:49.058886Z"
    },
    "papermill": {
     "duration": 0.020391,
     "end_time": "2024-04-23T16:17:49.061772",
     "exception": false,
     "start_time": "2024-04-23T16:17:49.041381",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SavedConfig:\n",
    "    \"\"\"\n",
    "    저장된 json 파일에서 설정을 로드하기 위한 플레이스홀더 클래스입니다.\n",
    "    \"\"\"\n",
    "    def __init__(self, dic):\n",
    "        # 딕셔너리의 각 항목을 객체의 속성으로 설정합니다.\n",
    "        for k, v in dic.items():\n",
    "            setattr(self, k, v)\n",
    "\n",
    "def load_model(path, model_name, override_backbone=None):\n",
    "    \"\"\"\n",
    "    이전에 훈련된 모델을 로드합니다.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path : str\n",
    "        모델이 저장된 경로\n",
    "    model_name : str\n",
    "        로드할 모델의 이름\n",
    "    override_backbone : str, optional\n",
    "        백본을 덮어쓸 경우 사용할 백본의 경로\n",
    "    \"\"\"\n",
    "    # 저장된 설정을 가져옵니다.\n",
    "    with open(os.path.join(path, \"config.json\"), \"r\") as f:\n",
    "        saved_configs = json.load(f)\n",
    "\n",
    "    # 저장된 설정을 SavedConfig 객체로 변환합니다.\n",
    "    saved_configs = SavedConfig(saved_configs)\n",
    "    \n",
    "    if override_backbone is not None:\n",
    "        # 백본을 덮어쓸 경우 설정을 업데이트합니다.\n",
    "        saved_configs.architecture = {\"backbone\": override_backbone,\n",
    "                             \"params\": {}}\n",
    "    # 네트워크 생성\n",
    "    network = CustomLLM(saved_configs, saved_configs.eos_token_id)\n",
    "    # 훈련된 가중치를 로드합니다.\n",
    "    state_dict = torch.load(os.path.join(path, f\"{model_name}.pt\"))\n",
    "\n",
    "    network.load_state_dict(state_dict)  # 네트워크에 가중치를 로드합니다.\n",
    "\n",
    "    # 모델 생성\n",
    "    clf = AbstractBaseModel(network=network,\n",
    "                            mixed_precision=saved_configs.mixed_precision)\n",
    "    clf.network.eval()  # 모델을 평가 모드로 설정합니다.\n",
    "    return saved_configs, clf  # 설정과 모델을 반환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d05a784",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-23T16:17:49.078485Z",
     "iopub.status.busy": "2024-04-23T16:17:49.077801Z",
     "iopub.status.idle": "2024-04-23T16:19:17.531745Z",
     "shell.execute_reply": "2024-04-23T16:19:17.530580Z"
    },
    "papermill": {
     "duration": 88.464987,
     "end_time": "2024-04-23T16:19:17.534302",
     "exception": false,
     "start_time": "2024-04-23T16:17:49.069315",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_PATH = \"/kaggle/input/simple-model-training/2/\"  # 사전 훈련된 모델이 저장된 경로\n",
    "MODEL_NAME = \"fold_0\"  # 로드할 모델의 이름\n",
    "\n",
    "if INFERENCE:\n",
    "    # 저장된 설정과 모델을 로드합니다.\n",
    "    # 내 모델은 로컬 머신에서 MAX_LENGTH=1024로 훈련되었습니다.\n",
    "    # Kaggle 노트북에서 훈련하지 않았기 때문에 백본을 덮어써야 합니다.\n",
    "    saved_configs, saved_model = load_model(MODEL_PATH, MODEL_NAME, override_backbone=\"/kaggle/input/gemma/transformers/1.1-2b-it/1\")\n",
    "    # 메모리 사용을 제한하기 위해 배치 크기를 1로 설정합니다.\n",
    "    saved_configs.batch_size = 1\n",
    "    \n",
    "    df_test = pd.read_csv(os.path.join(PATH_TO_DATA, \"test.csv\"))  # 테스트 데이터를 로드합니다.\n",
    "    # train.csv와 일치하도록 더미 'score' 열을 생성합니다.\n",
    "    df_test[\"score\"] = -1\n",
    "    # 테스트 데이터셋과 데이터로더를 생성합니다.\n",
    "    ds_test, dl_test = get_dataset_and_loader(df_test, saved_configs, inference=True)\n",
    "    \n",
    "    test_preds = saved_model.predict_proba(dl_test).numpy()  # 예측 확률을 계산합니다.\n",
    "    df_sub = pd.DataFrame()\n",
    "    df_sub[\"essay_id\"] = df_test[\"essay_id\"]\n",
    "    # 예측을 정수로 변환합니다.\n",
    "    df_sub[\"score\"] = np.rint(test_preds).astype(int)\n",
    "    # 제출 파일을 저장합니다.\n",
    "    df_sub.to_csv(\"submission.csv\", index=None)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 8059942,
     "sourceId": 71485,
     "sourceType": "competition"
    },
    {
     "datasetId": 4860630,
     "sourceId": 8204502,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4860826,
     "sourceId": 8207741,
     "sourceType": "datasetVersion"
    },
    {
     "modelInstanceId": 22003,
     "sourceId": 26140,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 136.32096,
   "end_time": "2024-04-23T16:19:20.318419",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-04-23T16:17:03.997459",
   "version": "2.5.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "060b006c462243899b00733d6004c935": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "14f9907a88294fb2bb8bef1cb951104d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_47822e0b3f494234aa36ca5e32d8b2b8",
       "placeholder": "​",
       "style": "IPY_MODEL_fcfe82cdaef64fc389d89db3fe29e2b9",
       "value": "Inference: 100%"
      }
     },
     "28b254dfad4d46fc8c848af97ce3fca3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_df97dd2224f34a849c7beb1b3cd174ce",
        "IPY_MODEL_c6571694c588405c8df4efb266bdd3a6",
        "IPY_MODEL_aec78b5b827b41f4ace9c54af9c2fbe7"
       ],
       "layout": "IPY_MODEL_a3487b00a5d94b85a102298b3690a158"
      }
     },
     "47822e0b3f494234aa36ca5e32d8b2b8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4bc83e9a933846829100649074f1d655": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "612438fdf51e4583a587ad80b0d9a216": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_14f9907a88294fb2bb8bef1cb951104d",
        "IPY_MODEL_c9f4f9f9d3894af6a57bcd0ab9206b82",
        "IPY_MODEL_8852c190b04242e39649deb62ea857a5"
       ],
       "layout": "IPY_MODEL_7b978c52d2f14b868a8a2d88affc37e5"
      }
     },
     "66b3446fff5444699fa6b13a1800402a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "6eb01a79d5a848b6801b4d108e7142f6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7b978c52d2f14b868a8a2d88affc37e5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": "hidden",
       "width": null
      }
     },
     "7cd53b6776eb4a7aa76a36f3293b12d0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8852c190b04242e39649deb62ea857a5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_db40134bd8a2411e869b30fdbb940f69",
       "placeholder": "​",
       "style": "IPY_MODEL_a21ddeb4b3dd4cd0ba9c8ae5109f0e9f",
       "value": " 3/3 [00:03&lt;00:00,  1.09it/s]"
      }
     },
     "a21ddeb4b3dd4cd0ba9c8ae5109f0e9f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "a3487b00a5d94b85a102298b3690a158": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a9c583a355ec4f42b91fb13772db3d1e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "aec78b5b827b41f4ace9c54af9c2fbe7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_7cd53b6776eb4a7aa76a36f3293b12d0",
       "placeholder": "​",
       "style": "IPY_MODEL_66b3446fff5444699fa6b13a1800402a",
       "value": " 2/2 [00:34&lt;00:00, 14.33s/it]"
      }
     },
     "b0386cf7ba8a4fe48fc59420fa863955": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "c30389e035d94defbaabe56c1ac46b97": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c6571694c588405c8df4efb266bdd3a6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_a9c583a355ec4f42b91fb13772db3d1e",
       "max": 2,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_060b006c462243899b00733d6004c935",
       "value": 2
      }
     },
     "c9f4f9f9d3894af6a57bcd0ab9206b82": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_c30389e035d94defbaabe56c1ac46b97",
       "max": 3,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_b0386cf7ba8a4fe48fc59420fa863955",
       "value": 3
      }
     },
     "db40134bd8a2411e869b30fdbb940f69": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "df97dd2224f34a849c7beb1b3cd174ce": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_6eb01a79d5a848b6801b4d108e7142f6",
       "placeholder": "​",
       "style": "IPY_MODEL_4bc83e9a933846829100649074f1d655",
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "fcfe82cdaef64fc389d89db3fe29e2b9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
